{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"},{"sourceId":120000,"sourceType":"modelInstanceVersion","modelInstanceId":100931,"modelId":121027}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-01T03:27:17.371067Z","iopub.execute_input":"2024-11-01T03:27:17.371447Z","iopub.status.idle":"2024-11-01T03:27:17.751466Z","shell.execute_reply.started":"2024-11-01T03:27:17.371410Z","shell.execute_reply":"2024-11-01T03:27:17.750259Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n/kaggle/input/llama-3.2/transformers/1b/1/config.json\n/kaggle/input/llama-3.2/transformers/1b/1/README.md\n/kaggle/input/llama-3.2/transformers/1b/1/USE_POLICY.md\n/kaggle/input/llama-3.2/transformers/1b/1/tokenizer.json\n/kaggle/input/llama-3.2/transformers/1b/1/tokenizer_config.json\n/kaggle/input/llama-3.2/transformers/1b/1/LICENSE.txt\n/kaggle/input/llama-3.2/transformers/1b/1/model.safetensors\n/kaggle/input/llama-3.2/transformers/1b/1/special_tokens_map.json\n/kaggle/input/llama-3.2/transformers/1b/1/.gitattributes\n/kaggle/input/llama-3.2/transformers/1b/1/generation_config.json\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"%%capture\n\n# Install and update all the necessary Python packages\n%pip install -U transformers \n%pip install -U datasets \n%pip install -U accelerate \n%pip install -U peft \n%pip install -U trl \n%pip install -U bitsandbytes \n%pip install -U wandb==0.17.8","metadata":{"execution":{"iopub.status.busy":"2024-11-01T03:27:18.768613Z","iopub.execute_input":"2024-11-01T03:27:18.769164Z","iopub.status.idle":"2024-11-01T03:29:15.561129Z","shell.execute_reply.started":"2024-11-01T03:27:18.769110Z","shell.execute_reply":"2024-11-01T03:29:15.559980Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Load the Python packages and functions for fine-tuning and evaluation\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\nimport os, torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, setup_chat_format\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()","metadata":{"execution":{"iopub.status.busy":"2024-11-01T03:29:19.057818Z","iopub.execute_input":"2024-11-01T03:29:19.058739Z","iopub.status.idle":"2024-11-01T03:29:37.452785Z","shell.execute_reply.started":"2024-11-01T03:29:19.058695Z","shell.execute_reply":"2024-11-01T03:29:37.451946Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Initialize wandb project for experimental tracking\nsecret_value_0 = user_secrets.get_secret(\"wandb\")\nwandb.login(key=secret_value_0)\nrun = wandb.init(project='fine-tuning-llama-models', job_type=\"training\", anonymous=\"allow\", settings=wandb.Settings(start_method=\"thread\"))","metadata":{"execution":{"iopub.status.busy":"2024-11-01T03:29:37.454553Z","iopub.execute_input":"2024-11-01T03:29:37.455618Z","iopub.status.idle":"2024-11-01T03:29:56.554392Z","shell.execute_reply.started":"2024-11-01T03:29:37.455567Z","shell.execute_reply":"2024-11-01T03:29:56.553281Z"},"trusted":true},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33movertheskyy\u001b[0m (\u001b[33movertheskyy-workspaces\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.8"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241101_032939-qod55xh4</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/overtheskyy-workspaces/fine-tuning-llama-models/runs/qod55xh4' target=\"_blank\">glittering-moon-19</a></strong> to <a href='https://wandb.ai/overtheskyy-workspaces/fine-tuning-llama-models' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/overtheskyy-workspaces/fine-tuning-llama-models' target=\"_blank\">https://wandb.ai/overtheskyy-workspaces/fine-tuning-llama-models</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/overtheskyy-workspaces/fine-tuning-llama-models/runs/qod55xh4' target=\"_blank\">https://wandb.ai/overtheskyy-workspaces/fine-tuning-llama-models/runs/qod55xh4</a>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Set the variables for base mode, dataset, and new model name\nbase_model = \"/kaggle/input/llama-3.2/transformers/1b/1\"\nnew_model = \"llama-3.2-1b-nlp-DisasterTweets\"\ndataset_name = \"/kaggle/input/nlp-getting-started/train.csv\"","metadata":{"execution":{"iopub.status.busy":"2024-11-01T03:31:04.175780Z","iopub.execute_input":"2024-11-01T03:31:04.176855Z","iopub.status.idle":"2024-11-01T03:31:04.182757Z","shell.execute_reply.started":"2024-11-01T03:31:04.176809Z","shell.execute_reply":"2024-11-01T03:31:04.181425Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Check if base model and dataset exist\nbase_model_exists = os.path.exists(base_model)\ndataset_exists = os.path.exists(dataset_name)\n\nprint(f\"Base model directory exists: {base_model_exists}\")\nprint(f\"Dataset file exists: {dataset_exists}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-01T03:31:05.289172Z","iopub.execute_input":"2024-11-01T03:31:05.289546Z","iopub.status.idle":"2024-11-01T03:31:05.299707Z","shell.execute_reply.started":"2024-11-01T03:31:05.289512Z","shell.execute_reply":"2024-11-01T03:31:05.298634Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Base model directory exists: True\nDataset file exists: True\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# # Using QLoRA config\n# # Set torch dtype and attention implementation\n# if torch.cuda.get_device_capability()[0] >= 8:\n#     !pip install -qqq flash-attn\n#     torch_dtype = torch.bfloat16\n#     attn_implementation = \"flash_attention_2\"\n# else:\n#     torch_dtype = torch.float16\n#     attn_implementation = \"eager\"\n\n# # QLoRA config\n# bnb_config = BitsAndBytesConfig(\n#     load_in_4bit=True,\n#     bnb_4bit_quant_type=\"nf4\",\n#     bnb_4bit_compute_dtype=torch_dtype,\n#     bnb_4bit_use_double_quant=True,\n# )\n# # Load model\n# model = AutoModelForCausalLM.from_pretrained(\n#     base_model,\n#     quantization_config=bnb_config,\n#     device_map=\"auto\",\n#     attn_implementation=attn_implementation\n# )","metadata":{"execution":{"iopub.status.busy":"2024-11-01T03:31:07.503631Z","iopub.execute_input":"2024-11-01T03:31:07.504311Z","iopub.status.idle":"2024-11-01T03:31:07.510778Z","shell.execute_reply.started":"2024-11-01T03:31:07.504271Z","shell.execute_reply":"2024-11-01T03:31:07.509705Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Load Base Model","metadata":{}},{"cell_type":"code","source":"# Load the pre-trained causal language model with specified configurations:\n# - `return_dict=True`: returns the output as a dictionary instead of a tuple.\n# - `low_cpu_mem_usage=True`: reduces CPU memory usage during model loading.\n# - `torch_dtype=torch.float16`: sets the model's data type to FP16 for memory efficiency.\n# - `device_map=\"auto\"`: automatically maps the model to available devices (e.g., GPU).\n# - `trust_remote_code=True`: allows execution of code from remote sources, useful for models with custom configurations.\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    return_dict=True,\n    low_cpu_mem_usage=True,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)","metadata":{"execution":{"iopub.status.busy":"2024-11-01T03:31:08.807887Z","iopub.execute_input":"2024-11-01T03:31:08.808734Z","iopub.status.idle":"2024-11-01T03:31:21.931552Z","shell.execute_reply.started":"2024-11-01T03:31:08.808691Z","shell.execute_reply":"2024-11-01T03:31:21.930501Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Checking the dataset\ntrain_data = pd.read_csv(dataset_name)\n\n# See the structure and data types of the columns\nprint(train_data.info())\n# Check for missing values\nprint(train_data.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2024-11-01T03:38:17.030121Z","iopub.execute_input":"2024-11-01T03:38:17.030764Z","iopub.status.idle":"2024-11-01T03:38:17.071736Z","shell.execute_reply.started":"2024-11-01T03:38:17.030722Z","shell.execute_reply":"2024-11-01T03:38:17.070700Z"},"trusted":true},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 7613 entries, 0 to 7612\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   id        7613 non-null   int64 \n 1   keyword   7552 non-null   object\n 2   location  5080 non-null   object\n 3   text      7613 non-null   object\n 4   target    7613 non-null   int64 \ndtypes: int64(2), object(3)\nmemory usage: 297.5+ KB\nNone\nid             0\nkeyword       61\nlocation    2533\ntext           0\ntarget         0\ndtype: int64\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Load Dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load dataset from CSV file\ndataset = load_dataset(\n    'csv',                   # Specify the format as CSV\n    data_files=dataset_name, # Path to the train CSV\n    split='train'            # Specify that you want to load the train split\n)\n\ndataset = dataset.remove_columns(['id', 'keyword', 'location'])\n\nprint(dataset.column_names)\n# print(\"First five entries:\", dataset[:5])\n\n# Create the new structure\nformatted_dataset = [{'text': text, 'label': target} for text, target in zip(dataset['text'], dataset['target'])]\n\n# Output the result\nprint(\"First five entries:\", formatted_dataset[:5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T03:59:20.078410Z","iopub.execute_input":"2024-11-01T03:59:20.078809Z","iopub.status.idle":"2024-11-01T03:59:20.226480Z","shell.execute_reply.started":"2024-11-01T03:59:20.078772Z","shell.execute_reply":"2024-11-01T03:59:20.225332Z"}},"outputs":[{"name":"stdout","text":"['text', 'target']\nFirst five entries: [{'text': 'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all', 'label': 1}, {'text': 'Forest fire near La Ronge Sask. Canada', 'label': 1}, {'text': \"All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\", 'label': 1}, {'text': '13,000 people receive #wildfires evacuation orders in California ', 'label': 1}, {'text': 'Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school ', 'label': 1}]\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}