{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"},{"sourceId":120000,"sourceType":"modelInstanceVersion","modelInstanceId":100931,"modelId":121027}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-31T19:02:45.923026Z","iopub.execute_input":"2024-10-31T19:02:45.923794Z","iopub.status.idle":"2024-10-31T19:02:45.931924Z","shell.execute_reply.started":"2024-10-31T19:02:45.923757Z","shell.execute_reply":"2024-10-31T19:02:45.930439Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/llama-3.2/transformers/1b/1/config.json\n/kaggle/input/llama-3.2/transformers/1b/1/README.md\n/kaggle/input/llama-3.2/transformers/1b/1/USE_POLICY.md\n/kaggle/input/llama-3.2/transformers/1b/1/tokenizer.json\n/kaggle/input/llama-3.2/transformers/1b/1/tokenizer_config.json\n/kaggle/input/llama-3.2/transformers/1b/1/LICENSE.txt\n/kaggle/input/llama-3.2/transformers/1b/1/model.safetensors\n/kaggle/input/llama-3.2/transformers/1b/1/special_tokens_map.json\n/kaggle/input/llama-3.2/transformers/1b/1/.gitattributes\n/kaggle/input/llama-3.2/transformers/1b/1/generation_config.json\n/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"%%capture\n\n# Install and update all the necessary Python packages\n%pip install -U transformers \n%pip install -U datasets \n%pip install -U accelerate \n%pip install -U peft \n%pip install -U trl \n%pip install -U bitsandbytes \n%pip install -U wandb==0.17.8","metadata":{"execution":{"iopub.status.busy":"2024-10-31T19:03:16.770218Z","iopub.execute_input":"2024-10-31T19:03:16.770597Z","iopub.status.idle":"2024-10-31T19:05:10.586302Z","shell.execute_reply.started":"2024-10-31T19:03:16.770560Z","shell.execute_reply":"2024-10-31T19:05:10.585226Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Load the Python packages and functions for fine-tuning and evaluation\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\nimport os, torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, setup_chat_format\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()","metadata":{"execution":{"iopub.status.busy":"2024-10-31T19:05:51.957243Z","iopub.execute_input":"2024-10-31T19:05:51.957658Z","iopub.status.idle":"2024-10-31T19:05:54.149296Z","shell.execute_reply.started":"2024-10-31T19:05:51.957621Z","shell.execute_reply":"2024-10-31T19:05:54.148496Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Initialize wandb project for experimental tracking\nsecret_value_0 = user_secrets.get_secret(\"wandb\")\nwandb.login(key=secret_value_0)\nrun = wandb.init(project='fine-tuning-llama-models', job_type=\"training\", anonymous=\"allow\", settings=wandb.Settings(start_method=\"thread\"))","metadata":{"execution":{"iopub.status.busy":"2024-10-31T19:05:57.961724Z","iopub.execute_input":"2024-10-31T19:05:57.962419Z","iopub.status.idle":"2024-10-31T19:06:16.676777Z","shell.execute_reply.started":"2024-10-31T19:05:57.962380Z","shell.execute_reply":"2024-10-31T19:06:16.675824Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33movertheskyy\u001b[0m (\u001b[33movertheskyy-workspaces\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.8"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241031_190600-t7w3ywg1</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/overtheskyy-workspaces/fine-tuning-llama-models/runs/t7w3ywg1' target=\"_blank\">paranormal-cauldron-17</a></strong> to <a href='https://wandb.ai/overtheskyy-workspaces/fine-tuning-llama-models' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/overtheskyy-workspaces/fine-tuning-llama-models' target=\"_blank\">https://wandb.ai/overtheskyy-workspaces/fine-tuning-llama-models</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/overtheskyy-workspaces/fine-tuning-llama-models/runs/t7w3ywg1' target=\"_blank\">https://wandb.ai/overtheskyy-workspaces/fine-tuning-llama-models/runs/t7w3ywg1</a>"},"metadata":{}}]},{"cell_type":"code","source":"# Set the variables for base mode, dataset, and new model name\nbase_model = \"/kaggle/input/llama-3.2/transformers/1b/1\"\nnew_model = \"llama-3.2-1b-nlp-DisasterTweets\"\ndataset_name = \"/kaggle/input/nlp-getting-started/train.csv\"","metadata":{"execution":{"iopub.status.busy":"2024-10-31T19:06:17.502288Z","iopub.execute_input":"2024-10-31T19:06:17.502691Z","iopub.status.idle":"2024-10-31T19:06:17.509154Z","shell.execute_reply.started":"2024-10-31T19:06:17.502645Z","shell.execute_reply":"2024-10-31T19:06:17.508038Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Check if base model and dataset exist\nbase_model_exists = os.path.exists(base_model)\ndataset_exists = os.path.exists(dataset_name)\n\nprint(f\"Base model directory exists: {base_model_exists}\")\nprint(f\"Dataset file exists: {dataset_exists}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-31T19:07:20.467661Z","iopub.execute_input":"2024-10-31T19:07:20.468290Z","iopub.status.idle":"2024-10-31T19:07:20.476677Z","shell.execute_reply.started":"2024-10-31T19:07:20.468251Z","shell.execute_reply":"2024-10-31T19:07:20.475536Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Base model directory exists: True\nDataset file exists: True\n","output_type":"stream"}]},{"cell_type":"code","source":"# # Using QLoRA config\n# # Set torch dtype and attention implementation\n# if torch.cuda.get_device_capability()[0] >= 8:\n#     !pip install -qqq flash-attn\n#     torch_dtype = torch.bfloat16\n#     attn_implementation = \"flash_attention_2\"\n# else:\n#     torch_dtype = torch.float16\n#     attn_implementation = \"eager\"\n\n# # QLoRA config\n# bnb_config = BitsAndBytesConfig(\n#     load_in_4bit=True,\n#     bnb_4bit_quant_type=\"nf4\",\n#     bnb_4bit_compute_dtype=torch_dtype,\n#     bnb_4bit_use_double_quant=True,\n# )\n# # Load model\n# model = AutoModelForCausalLM.from_pretrained(\n#     base_model,\n#     quantization_config=bnb_config,\n#     device_map=\"auto\",\n#     attn_implementation=attn_implementation\n# )","metadata":{"execution":{"iopub.status.busy":"2024-10-31T19:17:36.148230Z","iopub.execute_input":"2024-10-31T19:17:36.148889Z","iopub.status.idle":"2024-10-31T19:17:36.156487Z","shell.execute_reply.started":"2024-10-31T19:17:36.148847Z","shell.execute_reply":"2024-10-31T19:17:36.155528Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Load the pre-trained causal language model with specified configurations:\n# - `return_dict=True`: returns the output as a dictionary instead of a tuple.\n# - `low_cpu_mem_usage=True`: reduces CPU memory usage during model loading.\n# - `torch_dtype=torch.float16`: sets the model's data type to FP16 for memory efficiency.\n# - `device_map=\"auto\"`: automatically maps the model to available devices (e.g., GPU).\n# - `trust_remote_code=True`: allows execution of code from remote sources, useful for models with custom configurations.\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    return_dict=True,\n    low_cpu_mem_usage=True,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T19:31:41.648411Z","iopub.execute_input":"2024-10-31T19:31:41.649350Z","iopub.status.idle":"2024-10-31T19:31:44.789060Z","shell.execute_reply.started":"2024-10-31T19:31:41.649307Z","shell.execute_reply":"2024-10-31T19:31:44.787891Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Loading and processing the dataset\n\ntrain_data = pd.read_csv(dataset_name)\n\n# See the structure and data types of the columns\nprint(train_data.info())\n# Check for missing values\nprint(train_data.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2024-10-31T19:47:41.565367Z","iopub.execute_input":"2024-10-31T19:47:41.566248Z","iopub.status.idle":"2024-10-31T19:47:41.643409Z","shell.execute_reply.started":"2024-10-31T19:47:41.566209Z","shell.execute_reply":"2024-10-31T19:47:41.642361Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 7613 entries, 0 to 7612\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   id        7613 non-null   int64 \n 1   keyword   7552 non-null   object\n 2   location  5080 non-null   object\n 3   text      7613 non-null   object\n 4   target    7613 non-null   int64 \ndtypes: int64(2), object(3)\nmemory usage: 297.5+ KB\nNone\nid             0\nkeyword       61\nlocation    2533\ntext           0\ntarget         0\ndtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}